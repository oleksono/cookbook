{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T02:50:58.408908Z",
     "start_time": "2018-01-19T02:50:58.405402Z"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "<img src=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\" width=175px>\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Using the Keras library to import the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset due to the simplicity over other libraries (e.x. [CNTK](https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html#) and [TensorFlow](https://www.tensorflow.org/get_started/mnist/pros))\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\", width=400px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:38:49.501957Z",
     "start_time": "2018-01-19T23:38:21.725838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# Loading the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Ensuring the channels are in the correct \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'Training samples')\n",
    "print(X_test.shape[0], 'Testing samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T03:18:54.589991Z",
     "start_time": "2018-01-19T03:16:04.367713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/01/18 21:16\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy:  1.13.3\n",
      "Keras:  2.0.6\n",
      "Backend: TensorFlow 1.3.0\n",
      "[name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1524796620\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18240986228833159361\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              590848    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,732,586\n",
      "Trainable params: 1,732,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/12\n",
      "54000/54000 [==============================] - 24s - loss: 0.3952 - acc: 0.8678 - val_loss: 0.0571 - val_acc: 0.9823\n",
      "Epoch 2/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.1093 - acc: 0.9665 - val_loss: 0.0451 - val_acc: 0.9878\n",
      "Epoch 3/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.0814 - acc: 0.9741 - val_loss: 0.0413 - val_acc: 0.9887\n",
      "Epoch 4/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.0684 - acc: 0.9787 - val_loss: 0.0377 - val_acc: 0.9893\n",
      "Epoch 5/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.0606 - acc: 0.9809 - val_loss: 0.0315 - val_acc: 0.9907\n",
      "Epoch 6/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0525 - acc: 0.9840 - val_loss: 0.0317 - val_acc: 0.9915\n",
      "Epoch 7/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0495 - acc: 0.9845 - val_loss: 0.0283 - val_acc: 0.9920\n",
      "Epoch 8/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0470 - acc: 0.9854 - val_loss: 0.0264 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0433 - acc: 0.9863 - val_loss: 0.0300 - val_acc: 0.9917\n",
      "Epoch 10/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0391 - acc: 0.9875 - val_loss: 0.0264 - val_acc: 0.9930\n",
      "Epoch 11/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0373 - acc: 0.9883 - val_loss: 0.0240 - val_acc: 0.9935\n",
      "Epoch 12/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0362 - acc: 0.9890 - val_loss: 0.0256 - val_acc: 0.9937\n",
      " 9984/10000 [============================>.] - ETA: 0sTest score: 0.0188862003367\n",
      "Test accuracy: 0.9934\n",
      "Total training time for 12 epochs: 167 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import advanced_activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS: ', sys.platform)\n",
    "print('Python: ', sys.version)\n",
    "print('NumPy: ', np.__version__)\n",
    "print('Keras: ', keras.__version__)\n",
    "\n",
    "# Printing backend and GPU information\n",
    "if keras.backend.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.client import device_lib\n",
    "    print('Backend: TensorFlow', tf.__version__)\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print([x for x in local_device_protos if x.device_type == 'GPU'])\n",
    "\n",
    "    # Avoiding memory issues with the GPU\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "\n",
    "elif keras.backend.backend() == 'CNTK':\n",
    "    import cntk as C\n",
    "    print('Backend: CNTK', C.__version__)\n",
    "    print('GPU: ', c.gpu(0))\n",
    "\n",
    "\n",
    "# For tracking the training time for each epoch\n",
    "from keras import callbacks\n",
    "import time\n",
    "\n",
    "class TimeHistory(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Tracks training time on individual epochs for a Keras model\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Setting data types and normalizing the images\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'Training samples')\n",
    "print(X_test.shape[0], 'Testing samples')\n",
    "\n",
    "# Model settings\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# Converting class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Beginning model building\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1 - Conv (5x5)\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "\n",
    "# Layer 2 - Conv (5x5) & Max Pooling\n",
    "model.add(Conv2D(32, kernel_size=(5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 3 - Conv (3x3)\n",
    "model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "\n",
    "# Layer 4 - Conv (3x3) & Max Pooling\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 5 - FC 1024\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Layer 6 - FC 1024\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Defining loss function, optimizer, and metrics to report\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model before fitting\n",
    "model.summary()\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=128, verbose=1,\n",
    "          validation_split=0.1,  # Uses last 10% of data (not shuffled) for validation\n",
    "          callbacks=[time_callback])  # Gives epoch training times\n",
    "\n",
    "# Getting test information\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print()\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Saving the model\n",
    "# model.save('model.h5')\n",
    "\n",
    "# Reporting total training time\n",
    "total_training_time = round(sum(time_callback.times))\n",
    "print('Total training time for {0} epochs: {1} seconds'.format(epochs, total_training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Setup\n",
    "\n",
    "Splitting the training dataset into training and validation since Keras automatically does this when fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:38:50.251953Z",
     "start_time": "2018-01-19T23:38:49.517575Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:38:52.423847Z",
     "start_time": "2018-01-19T23:38:50.283202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/01/19 17:38\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy:  1.13.3\n",
      "CNTK:  2.1\n",
      "GPU:  GPU[0] GeForce GTX 680\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.layers import Convolution2D, MaxPooling, Dropout, Dense\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS: ', sys.platform)\n",
    "print('Python: ', sys.version)\n",
    "print('NumPy: ', np.__version__)\n",
    "print('CNTK: ', C.__version__)\n",
    "print('GPU: ', C.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference [1](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/ConvNet/Python/ConvNet_MNIST.py) | [2](https://cntk.ai/pythondocs/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.html) | [3](https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T00:04:40.286310Z",
     "start_time": "2018-01-20T00:04:40.134283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of the first convolution layer: (8, 14, 14)\n",
      "Bias value of the last dense layer: [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      "Training 1072922 parameters in 8 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "# Define the data dimensions\n",
    "input_dim_model = (1, 28, 28)  # Images are 28 x 28 with 1 channel of color (gray)\n",
    "input_dim = 28*28  # used by readers to treat input data as a vector\n",
    "num_output_classes = 10\n",
    "\n",
    "X = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)\n",
    "\n",
    "def create_model(input_vars, out_dims=10, dropout_prob=0.0):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "\n",
    "        conv1 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=8,\n",
    "                            strides=(2, 2),\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv1')(input_vars)\n",
    "        pooling1 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv1)\n",
    "\n",
    "        conv2 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=64,\n",
    "                            strides=1,\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv2')(pooling1)\n",
    "        pooling2 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv2)\n",
    "\n",
    "        fc1 = Dense(1024,\n",
    "                    init_bias=0.1,\n",
    "                    name='fc1')(pooling2)\n",
    "        \n",
    "        dropout1 = Dropout(dropout_prob)(fc1)\n",
    "        \n",
    "        output_layer = Dense(out_dims,\n",
    "                             activation=None,\n",
    "                             init_bias=0.1,\n",
    "                             name='classify')(dropout1)\n",
    "\n",
    "        return output_layer\n",
    "        \n",
    "# Defining the input to the neural network\n",
    "input_vars = C.input_variable(input_dim, np.float32)\n",
    "        \n",
    "# Create the model\n",
    "output = create_model(X)\n",
    "\n",
    "# Print the output shapes / parameters of different components\n",
    "print(\"Output Shape of the first convolution layer:\", output.conv1.shape)\n",
    "print(\"Bias value of the last dense layer:\", output.classify.b.value)\n",
    "\n",
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(output)\n",
    "\n",
    "### Setting up the trainer\n",
    "# Define the label as the other input parameter of the trainer\n",
    "labels = C.input_variable(num_output_classes, np.float32)\n",
    "\n",
    "# Initialize the parameters for the trainer\n",
    "train_minibatch_size = 50\n",
    "# lr_schedule = C.learners.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size=epoch_size)\n",
    "momentum = 0.9\n",
    "\n",
    "# Define the loss function\n",
    "loss = C.cross_entropy_with_softmax(output, labels)\n",
    "\n",
    "# Define the function that calculates classification error\n",
    "label_error = C.classification_error(output, labels)\n",
    "\n",
    "# Instantiate the trainer object to drive the model training\n",
    "# learner = C.learners.momentum_sgd(output.parameters, lr_schedule, momentum)\n",
    "# trainer = C.Trainer(output, loss, label_error, [learner])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T00:37:05.203151Z",
     "start_time": "2017-12-20T00:37:05.190651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T03:22:12.701700Z",
     "start_time": "2018-01-19T03:22:12.657967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/01/18 21:22\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy:  1.13.3\n",
      "TensorFlow:  1.3.0\n",
      "[name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 3341888561636021422\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS: ', sys.platform)\n",
    "print('Python: ', sys.version)\n",
    "print('NumPy: ', np.__version__)\n",
    "print('TensorFlow: ', tf.__version__)\n",
    "\n",
    "# Checking tensorflow processing devices\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x for x in local_device_protos if x.device_type == 'GPU'])\n",
    "\n",
    "# Avoiding memory issues with the GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
