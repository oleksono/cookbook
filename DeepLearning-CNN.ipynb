{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T02:50:58.408908Z",
     "start_time": "2018-01-19T02:50:58.405402Z"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "<img src=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\" width=175px>\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Using the Keras library to import the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset due to the simplicity over other libraries (e.x. [CNTK](https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html#) and [TensorFlow](https://www.tensorflow.org/get_started/mnist/pros))\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\", width=400px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T22:17:31.193213Z",
     "start_time": "2018-01-20T22:17:04.710135Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# Loading the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Ensuring the channels are in the correct \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'Training samples')\n",
    "print(X_test.shape[0], 'Testing samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T03:18:54.589991Z",
     "start_time": "2018-01-19T03:16:04.367713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/01/18 21:16\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy:  1.13.3\n",
      "Keras:  2.0.6\n",
      "Backend: TensorFlow 1.3.0\n",
      "[name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1524796620\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18240986228833159361\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              590848    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,732,586\n",
      "Trainable params: 1,732,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/12\n",
      "54000/54000 [==============================] - 24s - loss: 0.3952 - acc: 0.8678 - val_loss: 0.0571 - val_acc: 0.9823\n",
      "Epoch 2/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.1093 - acc: 0.9665 - val_loss: 0.0451 - val_acc: 0.9878\n",
      "Epoch 3/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.0814 - acc: 0.9741 - val_loss: 0.0413 - val_acc: 0.9887\n",
      "Epoch 4/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.0684 - acc: 0.9787 - val_loss: 0.0377 - val_acc: 0.9893\n",
      "Epoch 5/12\n",
      "54000/54000 [==============================] - 13s - loss: 0.0606 - acc: 0.9809 - val_loss: 0.0315 - val_acc: 0.9907\n",
      "Epoch 6/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0525 - acc: 0.9840 - val_loss: 0.0317 - val_acc: 0.9915\n",
      "Epoch 7/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0495 - acc: 0.9845 - val_loss: 0.0283 - val_acc: 0.9920\n",
      "Epoch 8/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0470 - acc: 0.9854 - val_loss: 0.0264 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0433 - acc: 0.9863 - val_loss: 0.0300 - val_acc: 0.9917\n",
      "Epoch 10/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0391 - acc: 0.9875 - val_loss: 0.0264 - val_acc: 0.9930\n",
      "Epoch 11/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0373 - acc: 0.9883 - val_loss: 0.0240 - val_acc: 0.9935\n",
      "Epoch 12/12\n",
      "54000/54000 [==============================] - 12s - loss: 0.0362 - acc: 0.9890 - val_loss: 0.0256 - val_acc: 0.9937\n",
      " 9984/10000 [============================>.] - ETA: 0sTest score: 0.0188862003367\n",
      "Test accuracy: 0.9934\n",
      "Total training time for 12 epochs: 167 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import advanced_activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('Keras:', keras.__version__)\n",
    "\n",
    "# Printing backend and GPU information\n",
    "if keras.backend.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.client import device_lib\n",
    "    print('Backend: TensorFlow', tf.__version__)\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print([x for x in local_device_protos if x.device_type == 'GPU'])\n",
    "\n",
    "    # Avoiding memory issues with the GPU\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "\n",
    "elif keras.backend.backend() == 'CNTK':\n",
    "    import cntk as C\n",
    "    print('Backend: CNTK', C.__version__)\n",
    "    print('GPU: ', c.gpu(0))\n",
    "\n",
    "\n",
    "# For tracking the training time for each epoch\n",
    "from keras import callbacks\n",
    "import time\n",
    "\n",
    "class TimeHistory(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Tracks training time on individual epochs for a Keras model\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Setting data types and normalizing the images\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'Training samples')\n",
    "print(X_test.shape[0], 'Testing samples')\n",
    "\n",
    "# Model settings\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# Converting class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Beginning model building\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1 - Conv (5x5)\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "\n",
    "# Layer 2 - Conv (5x5) & Max Pooling\n",
    "model.add(Conv2D(32, kernel_size=(5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 3 - Conv (3x3)\n",
    "model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "\n",
    "# Layer 4 - Conv (3x3) & Max Pooling\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 5 - FC 1024\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Layer 6 - FC 1024\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Defining loss function, optimizer, and metrics to report\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model before fitting\n",
    "model.summary()\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=128, verbose=1,\n",
    "          validation_split=0.1,  # Uses last 10% of data (not shuffled) for validation\n",
    "          callbacks=[time_callback])  # Gives epoch training times\n",
    "\n",
    "# Getting test information\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print()\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Saving the model\n",
    "# model.save('model.h5')\n",
    "\n",
    "# Reporting total training time\n",
    "total_training_time = round(sum(time_callback.times))\n",
    "print('Total training time for {0} epochs: {1} seconds'.format(epochs, total_training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Setup\n",
    "\n",
    "Splitting the training dataset into training and validation since Keras automatically does this when fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T22:17:31.881003Z",
     "start_time": "2018-01-20T22:17:31.200212Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T23:38:52.423847Z",
     "start_time": "2018-01-19T23:38:50.283202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/01/19 17:38\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy:  1.13.3\n",
      "CNTK:  2.1\n",
      "GPU:  GPU[0] GeForce GTX 680\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.layers import Convolution2D, MaxPooling, Dropout, Dense\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('CNTK:', C.__version__)\n",
    "print('GPU:', C.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference [1](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/ConvNet/Python/ConvNet_MNIST.py) | [2](https://cntk.ai/pythondocs/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.html) | [3](https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T00:04:40.286310Z",
     "start_time": "2018-01-20T00:04:40.134283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of the first convolution layer: (8, 14, 14)\n",
      "Bias value of the last dense layer: [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      "Training 1072922 parameters in 8 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "# Define the data dimensions\n",
    "input_dim_model = (1, 28, 28)  # Images are 28 x 28 with 1 channel of color (gray)\n",
    "input_dim = 28*28  # used by readers to treat input data as a vector\n",
    "num_output_classes = 10\n",
    "\n",
    "X = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)\n",
    "\n",
    "def create_model(input_vars, out_dims=10, dropout_prob=0.0):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "\n",
    "        conv1 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=8,\n",
    "                            strides=(2, 2),\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv1')(input_vars)\n",
    "        pooling1 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv1)\n",
    "\n",
    "        conv2 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=64,\n",
    "                            strides=1,\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv2')(pooling1)\n",
    "        pooling2 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv2)\n",
    "\n",
    "        fc1 = Dense(1024,\n",
    "                    init_bias=0.1,\n",
    "                    name='fc1')(pooling2)\n",
    "        \n",
    "        dropout1 = Dropout(dropout_prob)(fc1)\n",
    "        \n",
    "        output_layer = Dense(out_dims,\n",
    "                             activation=None,\n",
    "                             init_bias=0.1,\n",
    "                             name='classify')(dropout1)\n",
    "\n",
    "        return output_layer\n",
    "        \n",
    "# Defining the input to the neural network\n",
    "input_vars = C.input_variable(input_dim, np.float32)\n",
    "        \n",
    "# Create the model\n",
    "output = create_model(X)\n",
    "\n",
    "# Print the output shapes / parameters of different components\n",
    "print(\"Output Shape of the first convolution layer:\", output.conv1.shape)\n",
    "print(\"Bias value of the last dense layer:\", output.classify.b.value)\n",
    "\n",
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(output)\n",
    "\n",
    "### Setting up the trainer\n",
    "# Define the label as the other input parameter of the trainer\n",
    "labels = C.input_variable(num_output_classes, np.float32)\n",
    "\n",
    "# Initialize the parameters for the trainer\n",
    "train_minibatch_size = 50\n",
    "# lr_schedule = C.learners.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size=epoch_size)\n",
    "momentum = 0.9\n",
    "\n",
    "# Define the loss function\n",
    "loss = C.cross_entropy_with_softmax(output, labels)\n",
    "\n",
    "# Define the function that calculates classification error\n",
    "label_error = C.classification_error(output, labels)\n",
    "\n",
    "# Instantiate the trainer object to drive the model training\n",
    "# learner = C.learners.momentum_sgd(output.parameters, lr_schedule, momentum)\n",
    "# trainer = C.Trainer(output, loss, label_error, [learner])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T00:37:05.203151Z",
     "start_time": "2017-12-20T00:37:05.190651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/02/06 13:20\n",
      "OS: linux\n",
      "Python: 3.5.2 (default, Aug 18 2017, 17:48:00) \n",
      "[GCC 5.4.0 20160609]\n",
      "NumPy: 1.14.0\n",
      "PyTorch: 0.3.0.post4\n",
      "Epoch [1/5], Iter [100/600] Loss: 0.1559\n",
      "Epoch [1/5], Iter [200/600] Loss: 0.2541\n",
      "Epoch [1/5], Iter [300/600] Loss: 0.1078\n",
      "Epoch [1/5], Iter [400/600] Loss: 0.0419\n",
      "Epoch [1/5], Iter [500/600] Loss: 0.0725\n",
      "Epoch [1/5], Iter [600/600] Loss: 0.1366\n",
      "Epoch [2/5], Iter [100/600] Loss: 0.0406\n",
      "Epoch [2/5], Iter [200/600] Loss: 0.0233\n",
      "Epoch [2/5], Iter [300/600] Loss: 0.0631\n",
      "Epoch [2/5], Iter [400/600] Loss: 0.0380\n",
      "Epoch [2/5], Iter [500/600] Loss: 0.0049\n",
      "Epoch [2/5], Iter [600/600] Loss: 0.0495\n",
      "Epoch [3/5], Iter [100/600] Loss: 0.0103\n",
      "Epoch [3/5], Iter [200/600] Loss: 0.0652\n",
      "Epoch [3/5], Iter [300/600] Loss: 0.0153\n",
      "Epoch [3/5], Iter [400/600] Loss: 0.0650\n",
      "Epoch [3/5], Iter [500/600] Loss: 0.0205\n",
      "Epoch [3/5], Iter [600/600] Loss: 0.0374\n",
      "Epoch [4/5], Iter [100/600] Loss: 0.0126\n",
      "Epoch [4/5], Iter [200/600] Loss: 0.0348\n",
      "Epoch [4/5], Iter [300/600] Loss: 0.0123\n",
      "Epoch [4/5], Iter [400/600] Loss: 0.0465\n",
      "Epoch [4/5], Iter [500/600] Loss: 0.0342\n",
      "Epoch [4/5], Iter [600/600] Loss: 0.0205\n",
      "Epoch [5/5], Iter [100/600] Loss: 0.0088\n",
      "Epoch [5/5], Iter [200/600] Loss: 0.0023\n",
      "Epoch [5/5], Iter [300/600] Loss: 0.0108\n",
      "Epoch [5/5], Iter [400/600] Loss: 0.0845\n",
      "Epoch [5/5], Iter [500/600] Loss: 0.0307\n",
      "Epoch [5/5], Iter [600/600] Loss: 0.0223\n",
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('PyTorch:', torch.__version__)\n",
    "if torch.cuda.is_available() == True:\n",
    "    print('GPU:', torch.cuda.current_device())\n",
    "    \n",
    "    \n",
    "# Checking if there is a GPU and assigning it to a variable\n",
    "if torch.cuda.is_available() == True:\n",
    "    gpu = True\n",
    "else:\n",
    "    gpu = False\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# CNN Model (2 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "cnn = CNN()\n",
    "\n",
    "if gpu == True:\n",
    "    cnn.cuda()\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        if gpu == True:\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    if gpu == True:\n",
    "        images = Variable(images).cuda()\n",
    "    else:\n",
    "        images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100.0 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "# torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T22:29:59.790008Z",
     "start_time": "2018-01-20T22:29:34.813830Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/01/20 16:29\n",
      "OS:  win32\n",
      "Python:  3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy:  1.13.3\n",
      "TensorFlow:  1.3.0\n",
      "[name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 50533170\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 4328882277428310063\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "Reformatted data shapes:\n",
      "Training set (48000, 784) (48000, 10)\n",
      "Validation set (12000, 784) (12000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "\n",
      "Initialized\n",
      "\n",
      "Beginning Epoch 1 -\n",
      "Epoch 1 Step 0 (0.00% epoch, 0.00% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 73287.890625\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 18.0%\n",
      "2018-01-20 16:29:36.189824\n",
      "Total execution time: 0.00 minutes\n",
      "\n",
      "Epoch 1 Step 1000 (2.08% epoch, 0.16% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.308655\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 11.6%\n",
      "2018-01-20 16:29:45.538298\n",
      "Total execution time: 0.16 minutes\n",
      "\n",
      "Epoch 1 Step 2000 (4.17% epoch, 0.48% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.303080\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 9.5%\n",
      "2018-01-20 16:29:54.869082\n",
      "Total execution time: 0.31 minutes\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e25c142790b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m                          tf_train_labels: batch_labels}\n\u001b[0;32m    118\u001b[0m             _, l, sgd_hidden_predictions = sgd_hidden_session.run(\n\u001b[1;32m--> 119\u001b[1;33m                 [sgd_hidden_optimizer, sgd_hidden_loss, sgd_hidden_train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;31m# Calculating percentage of completion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('TensorFlow:', tf.__version__)\n",
    "\n",
    "# Checking tensorflow processing devices\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x for x in local_device_protos if x.device_type == 'GPU'])\n",
    "\n",
    "# Avoiding memory issues with the GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "\n",
    "X_train, y_train = reformat(X_train, y_train)\n",
    "X_validation, y_validation = reformat(X_validation, y_validation)\n",
    "X_test, y_test = reformat(X_test, y_test)\n",
    "print('Reformatted data shapes:')\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_validation.shape, y_validation.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = y_train.shape[0] + 1  # 200,000 per epoch\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "display_step = 250  # To print progress\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784  # Data input (image shape: 28x28)\n",
    "num_classes = 10  # Total classes (10 characters)\n",
    "\n",
    "num_hidden = 1024\n",
    "\n",
    "# Defining the model and graph\n",
    "sgd_hidden_graph = tf.Graph()\n",
    "with sgd_hidden_graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(X_validation.astype('float32'))\n",
    "    # ---------------------------------------> LABELED FOR LATER USE <----------------\n",
    "    tf_test_dataset = tf.constant(X_test, name='test_data')\n",
    "\n",
    "    w0 = tf.Variable(tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden]), name='W0')\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]), name='W1')\n",
    "\n",
    "    b0 = tf.Variable(tf.zeros([num_hidden]), name='b0')\n",
    "    b1 = tf.Variable(tf.zeros([num_labels]), name='b1')\n",
    "\n",
    "    def reluLayer(dataset):\n",
    "        return tf.nn.relu(tf.matmul(dataset, w0) + b0)\n",
    "\n",
    "    def logitLayer(dataset):\n",
    "        return tf.matmul(reluLayer(dataset), w1) + b1\n",
    "\n",
    "    sgd_hidden_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels,\n",
    "            logits=logitLayer(tf_train_dataset)))\n",
    "    sgd_hidden_optimizer = tf.train.GradientDescentOptimizer(\n",
    "        0.5).minimize(sgd_hidden_loss)\n",
    "\n",
    "    sgd_hidden_train_prediction = tf.nn.softmax(\n",
    "        logitLayer(tf_train_dataset), name='train_predictor')\n",
    "    sgd_hidden_valid_prediction = tf.nn.softmax(\n",
    "        logitLayer(tf_valid_dataset), name='validate_predictor')\n",
    "    sgd_hidden_test_prediction = tf.nn.softmax(\n",
    "        logitLayer(tf_test_dataset), name='test_predictor')\n",
    "\n",
    "\n",
    "# Creating the graph and running the model\n",
    "with tf.Session(graph=sgd_hidden_graph) as sgd_hidden_session:\n",
    "    saver = tf.train.Saver()\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # For tracking execution time and progress\n",
    "    start_time = time.time()\n",
    "    total_steps = 0\n",
    "    \n",
    "    print('\\nInitialized\\n')\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print('Beginning Epoch {0} -'.format(epoch))\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "            batch_data = X_train[offset:(offset + batch_size), :]\n",
    "            batch_labels = y_train[offset:(\n",
    "                offset + batch_size), :].reshape(batch_size, num_labels)\n",
    "            feed_dict = {tf_train_dataset: batch_data,\n",
    "                         tf_train_labels: batch_labels}\n",
    "            _, l, sgd_hidden_predictions = sgd_hidden_session.run(\n",
    "                [sgd_hidden_optimizer, sgd_hidden_loss, sgd_hidden_train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 1000 == 0) or (step == num_steps):\n",
    "                # Calculating percentage of completion\n",
    "                total_steps += step\n",
    "                pct_epoch = (step / float(num_steps)) * 100\n",
    "                pct_total = (total_steps / float(num_steps *\n",
    "                                                 (epochs + 1))) * 100  # Fix this line\n",
    "\n",
    "                # Printing progress\n",
    "                print('Epoch %d Step %d (%.2f%% epoch, %.2f%% total)' %\n",
    "                      (epoch, step, pct_epoch, pct_total))\n",
    "                print('------------------------------------')\n",
    "                print('Minibatch loss: %f' % l)\n",
    "                print(\"Minibatch accuracy: %.1f%%\" %\n",
    "                      accuracy(sgd_hidden_predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(sgd_hidden_valid_prediction.eval(),\n",
    "                                                               y_validation))\n",
    "                print(datetime.now())\n",
    "                print('Total execution time: %.2f minutes' % ((time.time() - start_time)/60.))\n",
    "                print()\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "        sgd_hidden_test_prediction.eval(), y_test.astype('int32')))\n",
    "#     saver.save(sgd_hidden_session, '{}/model'.format(Path.cwd()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
