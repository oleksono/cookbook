{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T02:50:58.408908Z",
     "start_time": "2018-01-19T02:50:58.405402Z"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "<img src=\"https://www.cntk.ai/jup/cntk103d_conv2d_final.gif\" width=175px>\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Using the Keras library to import the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset due to the simplicity over other libraries (e.x. [CNTK](https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html#) and [TensorFlow](https://www.tensorflow.org/get_started/mnist/pros))\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\", width=400px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T22:59:50.604338Z",
     "start_time": "2018-03-22T22:59:44.285085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# Loading the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_channels = 1\n",
    "\n",
    "# Ensuring the channels are in the correct \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], num_channels, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], num_channels, img_rows, img_cols)\n",
    "    input_shape = (num_channels, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, num_channels)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, num_channels)\n",
    "    input_shape = (img_rows, img_cols, num_channels)\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'Training samples')\n",
    "print(X_test.shape[0], 'Testing samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T18:40:23.263499Z",
     "start_time": "2018-03-18T18:37:05.569758Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/03/18 13:37\n",
      "OS: win32\n",
      "Python: 3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy: 1.13.3\n",
      "Keras: 2.0.6\n",
      "Backend: CNTK 2.3.1\n",
      "GPU:  GPU[0] GeForce GTX 680\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 Training samples\n",
      "10000 Testing samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              590848    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,732,586\n",
      "Trainable params: 1,732,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input254\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 17s - loss: 0.4200 - acc: 0.8587 - val_loss: 0.0606 - val_acc: 0.9808\n",
      "Epoch 2/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.1125 - acc: 0.9643 - val_loss: 0.0523 - val_acc: 0.9847\n",
      "Epoch 3/12\n",
      "54000/54000 [==============================] - 15s - loss: 0.0843 - acc: 0.9734 - val_loss: 0.0356 - val_acc: 0.9890\n",
      "Epoch 4/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0694 - acc: 0.9782 - val_loss: 0.0340 - val_acc: 0.9905\n",
      "Epoch 5/12\n",
      "54000/54000 [==============================] - 15s - loss: 0.0600 - acc: 0.9809 - val_loss: 0.0301 - val_acc: 0.9902\n",
      "Epoch 6/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0535 - acc: 0.9831 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "Epoch 7/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0500 - acc: 0.9848 - val_loss: 0.0325 - val_acc: 0.9905\n",
      "Epoch 8/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0468 - acc: 0.9857 - val_loss: 0.0297 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "54000/54000 [==============================] - 15s - loss: 0.0422 - acc: 0.9868 - val_loss: 0.0277 - val_acc: 0.9905\n",
      "Epoch 10/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0387 - acc: 0.9876 - val_loss: 0.0278 - val_acc: 0.9915\n",
      "Epoch 11/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0379 - acc: 0.9884 - val_loss: 0.0252 - val_acc: 0.9917\n",
      "Epoch 12/12\n",
      "54000/54000 [==============================] - 16s - loss: 0.0350 - acc: 0.9889 - val_loss: 0.0260 - val_acc: 0.9923\n",
      " 9856/10000 [============================>.] - ETA: 0s\n",
      "Test score: 0.0200272614799\n",
      "Test accuracy: 0.9931\n",
      "Total training time for 12 epochs: 195 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import advanced_activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('Keras:', keras.__version__)\n",
    "\n",
    "# Printing backend and GPU information\n",
    "if keras.backend.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.client import device_lib\n",
    "    print('Backend: TensorFlow', tf.__version__)\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print([x for x in local_device_protos if x.device_type == 'GPU'])\n",
    "\n",
    "    # Avoiding memory issues with the GPU\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "\n",
    "elif keras.backend.backend() == 'cntk':\n",
    "    import cntk as C\n",
    "    print('Backend: CNTK', C.__version__)\n",
    "    print('GPU: ', C.gpu(0))\n",
    "\n",
    "# Setting data types and normalizing the images\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'Training samples')\n",
    "print(X_test.shape[0], 'Testing samples')\n",
    "\n",
    "# Model settings\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# Converting class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Beginning model building\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1 - Conv (5x5)\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "\n",
    "# Layer 2 - Conv (5x5) & Max Pooling\n",
    "model.add(Conv2D(32, kernel_size=(5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 3 - Conv (3x3)\n",
    "model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "\n",
    "# Layer 4 - Conv (3x3) & Max Pooling\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 5 - FC 1024\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Layer 6 - FC 1024\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Defining loss function, optimizer, and metrics to report\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model before fitting\n",
    "model.summary()\n",
    "\n",
    "# Setting up early stopping\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                    min_delta=0.0001,  # Amount counting as an improvement\n",
    "                                    patience=5,  # Number of epochs before stopping\n",
    "                                    verbose=1, \n",
    "                                    mode='auto')\n",
    "\n",
    "# For tracking the training time for each epoch\n",
    "import time\n",
    "\n",
    "class TimeHistory(callbacks.Callback):\n",
    "    '''\n",
    "    Tracks training time on individual epochs for a Keras model\n",
    "    '''\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "time_callback = TimeHistory()  # Gives training time for all epochs\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size, verbose=1,\n",
    "          validation_split=0.1,  # Uses last 10% of data (not shuffled) for validation\n",
    "          callbacks=[earlystop, time_callback])\n",
    "\n",
    "# Getting test information\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print()\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Saving the model\n",
    "# model.save('model.h5')\n",
    "\n",
    "# Reporting total training time\n",
    "total_training_time = round(sum(time_callback.times))\n",
    "print('Total training time for {0} epochs: {1} seconds'.format(epochs, total_training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Using Keras functionality for training on augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagenerator = ImageDataGenerator(\n",
    "        rotation_range=360,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=None,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "datagenerator.fit(X_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model_info = model.fit_generator(generator=datagenerator.flow(X_train, y_train,\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              shuffle=True),\n",
    "                                 steps_per_epoch=10*round(X_train.shape[0] / batch_size),\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(X_val, y_val),\n",
    "                                 verbose=1,\n",
    "                                 callbacks=[time_callback,  # Gives epoch training times\n",
    "                                            earlystop,\n",
    "                                            callbacks.ModelCheckpoint('model.h5', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Setup\n",
    "\n",
    "Splitting the training dataset into training and validation since Keras automatically does this when fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T23:00:06.563570Z",
     "start_time": "2018-03-22T23:00:04.559751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK\n",
    "\n",
    "**To-do: **\n",
    "- Finish CNTK script to train the model\n",
    "- Add script for loading trained model and generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T23:00:06.619571Z",
     "start_time": "2018-03-22T23:00:06.597071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/03/22 18:00\n",
      "OS: win32\n",
      "Python: 3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy: 1.13.3\n",
      "CNTK: 2.3.1\n",
      "GPU: GPU[0] GeForce GTX 680\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.layers import Convolution2D, MaxPooling, Dropout, Dense\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('CNTK:', C.__version__)\n",
    "print('GPU:', C.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference [1](https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/ConvNet/Python/ConvNet_MNIST.py) | [2](https://cntk.ai/pythondocs/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.html) | [3](https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-20T00:04:40.286310Z",
     "start_time": "2018-01-20T00:04:40.134283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of the first convolution layer: (8, 14, 14)\n",
      "Bias value of the last dense layer: [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      "Training 1072922 parameters in 8 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "# Define the data dimensions\n",
    "input_dim_model = (1, 28, 28)  # Images are 28 x 28 with 1 channel of color (gray)\n",
    "input_dim = 28*28  # used by readers to treat input data as a vector\n",
    "num_output_classes = 10\n",
    "\n",
    "X = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)\n",
    "\n",
    "def create_model(input_vars, out_dims=10, dropout_prob=0.0):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "\n",
    "        conv1 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=8,\n",
    "                            strides=(2, 2),\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv1')(input_vars)\n",
    "        pooling1 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv1)\n",
    "\n",
    "        conv2 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=64,\n",
    "                            strides=1,\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv2')(pooling1)\n",
    "        pooling2 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv2)\n",
    "\n",
    "        fc1 = Dense(1024,\n",
    "                    init_bias=0.1,\n",
    "                    name='fc1')(pooling2)\n",
    "        \n",
    "        dropout1 = Dropout(dropout_prob)(fc1)\n",
    "        \n",
    "        output_layer = Dense(out_dims,\n",
    "                             activation=None,\n",
    "                             init_bias=0.1,\n",
    "                             name='classify')(dropout1)\n",
    "\n",
    "        return output_layer\n",
    "        \n",
    "# Defining the input to the neural network\n",
    "input_vars = C.input_variable(input_dim, np.float32)\n",
    "        \n",
    "# Create the model\n",
    "output = create_model(X)\n",
    "\n",
    "# Print the output shapes / parameters of different components\n",
    "print('Output Shape of the first convolution layer:', output.conv1.shape)\n",
    "print('Bias value of the last dense layer:', output.classify.b.value)\n",
    "\n",
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(output)\n",
    "\n",
    "### Setting up the trainer\n",
    "# Define the label as the other input parameter of the trainer\n",
    "labels = C.input_variable(num_output_classes, np.float32)\n",
    "\n",
    "# Initialize the parameters for the trainer\n",
    "train_minibatch_size = 50\n",
    "# lr_schedule = C.learners.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size=epoch_size)\n",
    "momentum = 0.9\n",
    "\n",
    "# Define the loss function\n",
    "loss = C.cross_entropy_with_softmax(output, labels)\n",
    "\n",
    "# Define the function that calculates classification error\n",
    "label_error = C.classification_error(output, labels)\n",
    "\n",
    "# Instantiate the trainer object to drive the model training\n",
    "# learner = C.learners.momentum_sgd(output.parameters, lr_schedule, momentum)\n",
    "# trainer = C.Trainer(output, loss, label_error, [learner])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Below is non-working code being saved here so I can use pull this notebook on other machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T00:06:24.177900Z",
     "start_time": "2018-03-23T00:06:24.129899Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    s = np.arange(len(X))\n",
    "    np.random.shuffle(s)\n",
    "    X = X[s]\n",
    "    y = y[s]\n",
    "    return X, y\n",
    "\n",
    "def yield_mb(X, y, batchsize=128, shuffle=False):\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y)\n",
    "    # Only complete batches are submitted\n",
    "    for i in range(len(X) // batchsize):\n",
    "        yield X[i * batchsize:(i + 1) * batchsize], y[i * batchsize:(i + 1) * batchsize]\n",
    "\n",
    "\n",
    "# def create_symbol(n_classes=10):\n",
    "#     # Weight initialiser from uniform distribution\n",
    "#     # Activation (unless states) is None\n",
    "#     with C.layers.default_options(init = C.glorot_uniform(), activation = C.relu):\n",
    "#         x = Convolution2D(filter_shape=(5, 5), num_filters=32, pad=True)(features)\n",
    "#         x = Convolution2D(filter_shape=(5, 5), num_filters=32, pad=True)(x)\n",
    "#         x = MaxPooling((2, 2), strides=(2, 2), pad=False)(x)\n",
    "#         x = Dropout(0.25)(x)\n",
    "\n",
    "#         x = Convolution2D(filter_shape=(3, 3), num_filters=100, pad=True)(x)\n",
    "#         x = Convolution2D(filter_shape=(3, 3), num_filters=100, pad=True)(x)\n",
    "#         x = MaxPooling((2, 2), strides=(2, 2), pad=False)(x)\n",
    "#         x = Dropout(0.25)(x)    \n",
    "        \n",
    "#         x = Dense(512)(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "#         x = Dense(n_classes, activation=None)(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# X = C.input_variable(input_dim_model)\n",
    "# y = C.input_variable(num_output_classes)\n",
    "\n",
    "def create_symbol(out_dims=10, dropout_prob=0.0):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "        conv1 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=8,\n",
    "                            strides=(2, 2),\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv1')(features)\n",
    "        pooling1 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv1)\n",
    "\n",
    "        conv2 = Convolution2D(filter_shape=(5, 5),\n",
    "                            num_filters=64,\n",
    "                            strides=1,\n",
    "                            pad=True,\n",
    "                            init_bias=0.1,\n",
    "                            name='conv2')(pooling1)\n",
    "        pooling2 = MaxPooling((2, 2),\n",
    "                              strides=(2, 2),\n",
    "                              pad=True)(conv2)\n",
    "\n",
    "        fc1 = Dense(1024,\n",
    "                    init_bias=0.1,\n",
    "                    name='fc1')(pooling2)\n",
    "        \n",
    "        dropout1 = Dropout(dropout_prob)(fc1)\n",
    "        \n",
    "        output_layer = Dense(out_dims,\n",
    "                             activation=None,\n",
    "                             init_bias=0.1,\n",
    "                             name='classify')(dropout1)\n",
    "\n",
    "        return output_layer\n",
    "    \n",
    "def init_model(m, labels, lr=LR, momentum=MOMENTUM):\n",
    "    # Loss (dense labels); check if support for sparse labels\n",
    "    loss = C.cross_entropy_with_softmax(m, labels)  \n",
    "    # Momentum SGD\n",
    "    # https://github.com/Microsoft/CNTK/blob/master/Manual/Manual_How_to_use_learners.ipynb\n",
    "    # unit_gain=False: momentum_direction = momentum*old_momentum_direction + gradient\n",
    "    # if unit_gain=True then ...(1-momentum)*gradient\n",
    "    learner = C.momentum_sgd(m.parameters,\n",
    "                                lr=C.learning_rate_schedule(lr, C.UnitType.minibatch),\n",
    "                                momentum=C.momentum_schedule(momentum), \n",
    "                                unit_gain=False)\n",
    "    trainer = C.Trainer(m, (loss, C.classification_error(m, labels)), [learner])\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# Placeholders\n",
    "# [128 x 28 x 28 x 1\n",
    "# features = C.input_variable((3, 32, 32), np.float32)\n",
    "features = C.input_variable((28, 28), np.float32)\n",
    "labels = C.input_variable(10, np.float32)\n",
    "# Load symbol\n",
    "sym = create_symbol()\n",
    "\n",
    "trainer = init_model(sym, labels)\n",
    "\n",
    "y_train_onehot = np.eye(len(np.unique(y_train)))[y_train]\n",
    "\n",
    "# Main training loop: 53s\n",
    "for j in range(3):\n",
    "    for data, label in yield_mb(X_train.astype('float32'), y_train_onehot.astype('float32'), batch_size, shuffle=True):\n",
    "        trainer.train_minibatch({features: data, labels: label})\n",
    "    # Log (this is just last batch in epoch, not average of batches)\n",
    "    eval_error = trainer.previous_minibatch_evaluation_average\n",
    "    print(\"Epoch %d  |  Accuracy: %.6f\" % (j+1, (1-eval_error)))\n",
    "    \n",
    "    \n",
    "# learner = C.sgd(model.parameters,\n",
    "#                 C.learning_parameter_schedule(0.1))\n",
    "# progress_writer = C.logging.ProgressPrinter(0)\n",
    "\n",
    "# train_summary = loss.train((X_train_lr, Y_train_lr), parameter_learners=[learner],\n",
    "#                    callbacks=[progress_writer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet\n",
    "\n",
    "**To-do:**\n",
    "- Add script for training a model in MXNet\n",
    "- Add script for loading a trained model and generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T00:37:05.203151Z",
     "start_time": "2017-12-20T00:37:05.190651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "**To-do:**\n",
    "- Conform model architecture to other models\n",
    "- Add script for loading a trained model and generating predictions\n",
    "- Add train/validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/02/06 13:20\n",
      "OS: linux\n",
      "Python: 3.5.2 (default, Aug 18 2017, 17:48:00) \n",
      "[GCC 5.4.0 20160609]\n",
      "NumPy: 1.14.0\n",
      "PyTorch: 0.3.0.post4\n",
      "Epoch [1/5], Iter [100/600] Loss: 0.1559\n",
      "Epoch [1/5], Iter [200/600] Loss: 0.2541\n",
      "Epoch [1/5], Iter [300/600] Loss: 0.1078\n",
      "Epoch [1/5], Iter [400/600] Loss: 0.0419\n",
      "Epoch [1/5], Iter [500/600] Loss: 0.0725\n",
      "Epoch [1/5], Iter [600/600] Loss: 0.1366\n",
      "Epoch [2/5], Iter [100/600] Loss: 0.0406\n",
      "Epoch [2/5], Iter [200/600] Loss: 0.0233\n",
      "Epoch [2/5], Iter [300/600] Loss: 0.0631\n",
      "Epoch [2/5], Iter [400/600] Loss: 0.0380\n",
      "Epoch [2/5], Iter [500/600] Loss: 0.0049\n",
      "Epoch [2/5], Iter [600/600] Loss: 0.0495\n",
      "Epoch [3/5], Iter [100/600] Loss: 0.0103\n",
      "Epoch [3/5], Iter [200/600] Loss: 0.0652\n",
      "Epoch [3/5], Iter [300/600] Loss: 0.0153\n",
      "Epoch [3/5], Iter [400/600] Loss: 0.0650\n",
      "Epoch [3/5], Iter [500/600] Loss: 0.0205\n",
      "Epoch [3/5], Iter [600/600] Loss: 0.0374\n",
      "Epoch [4/5], Iter [100/600] Loss: 0.0126\n",
      "Epoch [4/5], Iter [200/600] Loss: 0.0348\n",
      "Epoch [4/5], Iter [300/600] Loss: 0.0123\n",
      "Epoch [4/5], Iter [400/600] Loss: 0.0465\n",
      "Epoch [4/5], Iter [500/600] Loss: 0.0342\n",
      "Epoch [4/5], Iter [600/600] Loss: 0.0205\n",
      "Epoch [5/5], Iter [100/600] Loss: 0.0088\n",
      "Epoch [5/5], Iter [200/600] Loss: 0.0023\n",
      "Epoch [5/5], Iter [300/600] Loss: 0.0108\n",
      "Epoch [5/5], Iter [400/600] Loss: 0.0845\n",
      "Epoch [5/5], Iter [500/600] Loss: 0.0307\n",
      "Epoch [5/5], Iter [600/600] Loss: 0.0223\n",
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('PyTorch:', torch.__version__)\n",
    "if torch.cuda.is_available() == True:\n",
    "    print('GPU:', torch.cuda.current_device())\n",
    "    \n",
    "    \n",
    "# Checking if there is a GPU and assigning it to a variable\n",
    "if torch.cuda.is_available() == True:\n",
    "    gpu = True\n",
    "else:\n",
    "    gpu = False\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# CNN Model (2 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "cnn = CNN()\n",
    "\n",
    "if gpu == True:\n",
    "    cnn.cuda()\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        if gpu == True:\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    if gpu == True:\n",
    "        images = Variable(images).cuda()\n",
    "    else:\n",
    "        images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100.0 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "# torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "**To-do:**\n",
    "- Conform architecture to others in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-20T22:36:50.981175Z",
     "start_time": "2018-03-20T22:36:22.386624Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018/03/20 17:36\n",
      "OS: win32\n",
      "Python: 3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy: 1.13.3\n",
      "TensorFlow: 1.3.0\n",
      "[name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1524796620\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12763196243698338072\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "Reformatted data shapes:\n",
      "Training set (48000, 784) (48000, 10)\n",
      "Validation set (12000, 784) (12000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "\n",
      "Initialized\n",
      "\n",
      "Beginning Epoch 1 -\n",
      "Epoch 1 Step 0 (0.00% epoch, 0.00% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 62305.937500\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 10.2%\n",
      "2018-03-20 17:36:26.054341\n",
      "Total execution time: 0.01 minutes\n",
      "\n",
      "Epoch 1 Step 1000 (2.08% epoch, 0.16% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.296496\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 10.5%\n",
      "2018-03-20 17:36:29.352841\n",
      "Total execution time: 0.06 minutes\n",
      "\n",
      "Epoch 1 Step 2000 (4.17% epoch, 0.48% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.312936\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.8%\n",
      "2018-03-20 17:36:32.736199\n",
      "Total execution time: 0.12 minutes\n",
      "\n",
      "Epoch 1 Step 3000 (6.25% epoch, 0.96% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.294709\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 10.8%\n",
      "2018-03-20 17:36:36.377200\n",
      "Total execution time: 0.18 minutes\n",
      "\n",
      "Epoch 1 Step 4000 (8.33% epoch, 1.60% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.309429\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 10.2%\n",
      "2018-03-20 17:36:39.849201\n",
      "Total execution time: 0.24 minutes\n",
      "\n",
      "Epoch 1 Step 5000 (10.42% epoch, 2.40% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.294353\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 10.8%\n",
      "2018-03-20 17:36:43.187700\n",
      "Total execution time: 0.29 minutes\n",
      "\n",
      "Epoch 1 Step 6000 (12.50% epoch, 3.37% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.296662\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 9.8%\n",
      "2018-03-20 17:36:46.425175\n",
      "Total execution time: 0.34 minutes\n",
      "\n",
      "Epoch 1 Step 7000 (14.58% epoch, 4.49% total)\n",
      "------------------------------------\n",
      "Minibatch loss: 2.302536\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.8%\n",
      "2018-03-20 17:36:49.893675\n",
      "Total execution time: 0.40 minutes\n",
      "\n",
      "Training manually ended\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(time.strftime('%Y/%m/%d %H:%M'))\n",
    "print('OS:', sys.platform)\n",
    "print('Python:', sys.version)\n",
    "print('NumPy:', np.__version__)\n",
    "print('TensorFlow:', tf.__version__)\n",
    "\n",
    "# Checking tensorflow processing devices\n",
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print([x for x in local_device_protos if x.device_type == 'GPU'])\n",
    "\n",
    "# Avoiding memory issues with the GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "\n",
    "X_train, y_train = reformat(X_train, y_train)\n",
    "X_validation, y_validation = reformat(X_validation, y_validation)\n",
    "X_test, y_test = reformat(X_test, y_test)\n",
    "print('Reformatted data shapes:')\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_validation.shape, y_validation.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = y_train.shape[0] + 1  # 200,000 per epoch\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "display_step = 250  # To print progress\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784  # Data input (image shape: 28x28)\n",
    "num_classes = 10  # Total classes (10 characters)\n",
    "\n",
    "num_hidden = 1024\n",
    "\n",
    "# Defining the model and graph\n",
    "sgd_hidden_graph = tf.Graph()\n",
    "with sgd_hidden_graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(X_validation.astype('float32'))\n",
    "    tf_test_dataset = tf.constant(X_test, name='test_data')\n",
    "\n",
    "    w0 = tf.Variable(tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden]), name='W0')\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]), name='W1')\n",
    "\n",
    "    b0 = tf.Variable(tf.zeros([num_hidden]), name='b0')\n",
    "    b1 = tf.Variable(tf.zeros([num_labels]), name='b1')\n",
    "\n",
    "    def reluLayer(dataset):\n",
    "        return tf.nn.relu(tf.matmul(dataset, w0) + b0)\n",
    "\n",
    "    def logitLayer(dataset):\n",
    "        return tf.matmul(reluLayer(dataset), w1) + b1\n",
    "\n",
    "    sgd_hidden_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf_train_labels,\n",
    "            logits=logitLayer(tf_train_dataset)))\n",
    "    sgd_hidden_optimizer = tf.train.GradientDescentOptimizer(\n",
    "        0.5).minimize(sgd_hidden_loss)\n",
    "\n",
    "    sgd_hidden_train_prediction = tf.nn.softmax(\n",
    "        logitLayer(tf_train_dataset), name='train_predictor')\n",
    "    sgd_hidden_valid_prediction = tf.nn.softmax(\n",
    "        logitLayer(tf_valid_dataset), name='validate_predictor')\n",
    "    sgd_hidden_test_prediction = tf.nn.softmax(\n",
    "        logitLayer(tf_test_dataset), name='test_predictor')\n",
    "\n",
    "\n",
    "# Creating the graph and running the model\n",
    "with tf.Session(graph=sgd_hidden_graph) as sgd_hidden_session:\n",
    "    saver = tf.train.Saver()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # For tracking execution time and progress\n",
    "    start_time = time.time()\n",
    "    total_steps = 0\n",
    "\n",
    "    print('\\nInitialized\\n')\n",
    "    try:\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print('Beginning Epoch {0} -'.format(epoch))\n",
    "            for step in range(num_steps):\n",
    "                offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "                batch_data = X_train[offset:(offset + batch_size), :]\n",
    "                batch_labels = y_train[offset:(\n",
    "                    offset + batch_size), :].reshape(batch_size, num_labels)\n",
    "                feed_dict = {tf_train_dataset: batch_data,\n",
    "                             tf_train_labels: batch_labels}\n",
    "                _, l, sgd_hidden_predictions = sgd_hidden_session.run(\n",
    "                    [sgd_hidden_optimizer, sgd_hidden_loss, sgd_hidden_train_prediction],\n",
    "                    feed_dict=feed_dict)\n",
    "                if (step % 1000 == 0) or (step == num_steps):\n",
    "                    # Calculating percentage of completion\n",
    "                    total_steps += step\n",
    "                    pct_epoch = (step / float(num_steps)) * 100\n",
    "                    pct_total = (total_steps / float(num_steps *\n",
    "                                                     (epochs + 1))) * 100  # Fix this line\n",
    "\n",
    "                    # Printing progress\n",
    "                    print('Epoch %d Step %d (%.2f%% epoch, %.2f%% total)' %\n",
    "                          (epoch, step, pct_epoch, pct_total))\n",
    "                    print('------------------------------------')\n",
    "                    print('Minibatch loss: %f' % l)\n",
    "                    print(\"Minibatch accuracy: %.1f%%\" %\n",
    "                          accuracy(sgd_hidden_predictions, batch_labels))\n",
    "                    print(\"Validation accuracy: %.1f%%\" % accuracy(sgd_hidden_valid_prediction.eval(),\n",
    "                                                                   y_validation))\n",
    "                    print(datetime.now())\n",
    "                    print('Total execution time: %.2f minutes' %\n",
    "                          ((time.time() - start_time) / 60.))\n",
    "                    print()\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "            sgd_hidden_test_prediction.eval(), y_test.astype('int32')))\n",
    "    except KeyboardInterrupt:\n",
    "        print('Training manually ended')\n",
    "#     saver.save(sgd_hidden_session, '{}/model'.format(Path.cwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in a trained model and generating predictions\n",
    "\n",
    "Most of this script is from a competition submission script and needs to be cleaned up. Additionally, it contains code for creating a pickle file from images within subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "from scipy import ndimage\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "# Change this line to the saved model\n",
    "modelFile = 'trainedModel.meta'\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "num_labels = 10\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "total_images = sum([len(files) for r, d, files in os.walk(\"./data/\")])\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label.\"\"\"\n",
    "    image_files = list(folder.iterdir())\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = folder.joinpath(image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) -\n",
    "                          pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' %\n",
    "                                str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file,\n",
    "                  ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                        (num_images, min_num_images))\n",
    "\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    data_folders = (i for i in data_folders if i.is_dir())\n",
    "    for folder in data_folders:\n",
    "        set_filename = str(folder) + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names\n",
    "\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        labels.shape[0], image_size, image_size, 1).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def merge_datasets(pickle_files, dataset_size):\n",
    "    \"\"\"\n",
    "    Merge multiple glyph pickle files into nd-array dataset and nd-array labels\n",
    "    for model evaluation.\n",
    "    Simplification from https://github.com/udacity/deep-learning\n",
    "    \"\"\"\n",
    "    num_classes = len(pickle_files)\n",
    "    dataset, labels = make_arrays(dataset_size, image_size)\n",
    "    size_per_class = dataset_size // num_classes\n",
    "\n",
    "    start_t = 0\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                end_t = start_t + size_per_class\n",
    "                np.random.shuffle(letter_set)\n",
    "                dataset[start_t:end_t, :, :] = letter_set[0:size_per_class]\n",
    "                labels[start_t:end_t] = label\n",
    "                start_t = end_t\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "dir_name = 'data'\n",
    "\n",
    "glyph_dir = Path.cwd().joinpath(dir_name)\n",
    "test_folders = [glyph_dir.joinpath(i) for i in glyph_dir.iterdir()]\n",
    "test_datasets = maybe_pickle(test_folders, 2)  # provide only 2 samples for now\n",
    "test_dataset, test_labels = merge_datasets(test_datasets, total_images)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Testing size', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "def classify(test_dataset, model_filename=modelFile, checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    A sample classifier to unpickle a TensorFlow model and label the dataset.\n",
    "    There are magic strings in this function derived from to the model to evaluate.\n",
    "    Your implementation will likely have different tags that depend upon the model\n",
    "    implementation.\n",
    "\n",
    "    We pad the input test_dataset to make it at least as large as the model batch,\n",
    "    and repeat prediction on chunks of the input if it is larger than the model batch.\n",
    "\n",
    "    Args:\n",
    "        test_dataset: Expect an input of N*28*28*1 shape numpy array, where N is number of images and\n",
    "                      28*28 is pixel width and hieght.\n",
    "        model_filename: optional file name stored by a previous TensorFlow session.\n",
    "        checkpoint_path: optional path for previous TensorFlow session.\n",
    "\n",
    "    Returns:\n",
    "        The #observations by #labels nd-array labelings.\n",
    "    \"\"\"\n",
    "    # Re-scaling the dataset to a similar scale as the training data in the notMNIST.pickle file\n",
    "    pixel_depth = 255.0\n",
    "    test_dataset = (test_dataset - 255.0 / 2) / 255\n",
    "\n",
    "    num_classes = 10\n",
    "    n = int(test_dataset.shape[0])\n",
    "    result = np.ndarray([n, num_classes], dtype=np.float32)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        saver = tf.train.import_meta_graph('./model/' + model_filename)\n",
    "        saver.restore(session, './model/' + model_filename.split('.')[0])\n",
    "        graph_predict = tf.get_default_graph()\n",
    "        test_predict = graph_predict.get_tensor_by_name(\n",
    "            'Softmax_1:0')  # string from model\n",
    "        m = int(graph_predict.get_tensor_by_name(\n",
    "            'Placeholder:0').shape[0])   # string from model\n",
    "        for i in range(0, int(math.ceil(n / m))):\n",
    "            start = i * m\n",
    "            end = min(n, ((i + 1) * m))\n",
    "            x = np.zeros((128, 28, 28, 1)).astype(np.float32)\n",
    "            x[0:(end - start)] = test_dataset[start:end]\n",
    "            result[start:end] = test_predict.eval(\n",
    "                feed_dict={\"Placeholder:0\": x})[0:(end - start)]\n",
    "    return result\n",
    "\n",
    "\n",
    "def testing_accuracy(data, labels):\n",
    "    \"\"\"\n",
    "    Generates predictions and returns the accuracy\n",
    "    \"\"\"\n",
    "    predictions = classify(data)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(predictions, labels))\n",
    "\n",
    "\n",
    "testing_accuracy((test_dataset), test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
